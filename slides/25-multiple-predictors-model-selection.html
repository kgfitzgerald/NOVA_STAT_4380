<!DOCTYPE html>
<html lang="" xml:lang="">
  <head>
    <title>Models with multiple predictors</title>
    <meta charset="utf-8" />
    <meta name="author" content="Katie Fitzgerald, adpated from datasciencebox.org" />
    <script src="libs/header-attrs/header-attrs.js"></script>
    <link href="libs/font-awesome/css/all.min.css" rel="stylesheet" />
    <link href="libs/font-awesome/css/v4-shims.min.css" rel="stylesheet" />
    <link href="libs/panelset/panelset.css" rel="stylesheet" />
    <script src="libs/panelset/panelset.js"></script>
    <script src="libs/htmlwidgets/htmlwidgets.js"></script>
    <script src="libs/pymjs/pym.v1.js"></script>
    <script src="libs/widgetframe-binding/widgetframe.js"></script>
    <link rel="stylesheet" href="xaringan-themer.css" type="text/css" />
    <link rel="stylesheet" href="slides.css" type="text/css" />
  </head>
  <body>
    <textarea id="source">
class: center, middle, inverse, title-slide

.title[
# Models with multiple predictors
]
.subtitle[
## STAT 4380
]
.author[
### Katie Fitzgerald, adpated from datasciencebox.org
]

---





layout: true
  
&lt;div class="my-footer"&gt;
&lt;span&gt;
&lt;a href="https://nova-stat-4380.netlify.app" target="_blank"&gt;nova-stat-4380.netlify.app&lt;/a&gt;
&lt;/span&gt;
&lt;/div&gt; 

---



class: middle

# The linear model with multiple predictors

---

## Two numeric predictors: Paris paintings data



- Response variable: `log_price` 
- Explanatory variables: Width and height


``` r
pp_fit &lt;- lm(log_price ~ Width_in + Height_in, data = pp)
tidy(pp_fit)
```

```
## # A tibble: 3 × 5
##   term        estimate std.error statistic  p.value
##   &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;
## 1 (Intercept)   4.77     0.0579      82.4  0       
## 2 Width_in      0.0269   0.00373      7.22 6.58e-13
## 3 Height_in    -0.0133   0.00395     -3.36 7.93e- 4
```
&lt;br&gt;

`$$\widehat{log\_price} = 4.77 + 0.0269 \times width - 0.0133 \times height$$`

---

## Interpretation of estimates


```
## # A tibble: 3 × 5
##   term        estimate std.error statistic  p.value
##   &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;
## 1 (Intercept)   4.77     0.0579      82.4  0       
## 2 Width_in      0.0269   0.00373      7.22 6.58e-13
## 3 Height_in    -0.0133   0.00395     -3.36 7.93e- 4
```

--
- **Slope - Width_in:** *All else held constant*, for each additional inch wider a painting is, we would expect the log price to be higher, on average, by 0.0269. Or, we expect the price (in dollars) to be higher by a factor of `exp(0.0269)` = 1.03.

--
- **Slope - Height_in:** *All else held constant*, for each additional inch taller a painting is, we would expect the log price to be lower, on average, by 0.0133. Or, we expect the price (in dollars) to be lower by a factor of `exp(-0.0133)` = 0.99.

--
- **Intercept:** Paintings with 0 width and 0 height... (Doesn't make sense in context.)

---

## Visualizing models with multiple predictors

.panelset[
.panel[.panel-name[Plot]
.pull-left-wide[
<div id="htmlwidget-8f836166d559454ecd73" style="width:100%;height:1483.2px;" class="widgetframe html-widget"></div>
<script type="application/json" data-for="htmlwidget-8f836166d559454ecd73">{"x":{"url":"25-multiple-predictors-model-selection_files/figure-html//widgets/widget_plotly-plot.html","options":{"xdomain":"*","allowfullscreen":false,"lazyload":false}},"evals":[],"jsHooks":[]}</script>
]
]
.panel[.panel-name[Code]

``` r
p &lt;- plot_ly(pp,
  x = ~Width_in, y = ~Height_in, z = ~log_price,
  marker = list(size = 3, color = "lightgray", alpha = 0.5, 
                line = list(color = "gray", width = 2))) |&gt;
  add_markers() |&gt;
  plotly::layout(scene = list(
    xaxis = list(title = "Width (in)"),
    yaxis = list(title = "Height (in)"),
    zaxis = list(title = "log_price")
  )) |&gt;
  config(displayModeBar = FALSE)
frameWidget(p)
```
]
]

---

## Data: Book weight and volume

The `allbacks` data frame gives measurements on the volume and weight of 15 books, some of which are paperback and some of which are hardback

.pull-left[
- Volume - cubic centimetres
- Area - square centimetres
- Weight - grams
]
.pull-right[
.small[

```
## # A tibble: 15 × 4
##    volume  area weight cover
##     &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;fct&gt;
##  1    885   382    800 hb   
##  2   1016   468    950 hb   
##  3   1125   387   1050 hb   
##  4    239   371    350 hb   
##  5    701   371    750 hb   
##  6    641   367    600 hb   
##  7   1228   396   1075 hb   
##  8    412     0    250 pb   
##  9    953     0    700 pb   
## 10    929     0    650 pb   
## 11   1492     0    975 pb   
## 12    419     0    350 pb   
## 13   1010     0    950 pb   
## 14    595     0    425 pb   
## 15   1034     0    725 pb
```
]
]

.footnote[
.small[
These books are from the bookshelf of J. H. Maindonald at Australian National University.
]
]

---

## Book weight vs. volume

.pull-left[

``` r
lm(weight ~ volume, data = allbacks) |&gt;
  tidy()
```

```
## # A tibble: 2 × 5
##   term        estimate std.error statistic    p.value
##   &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;      &lt;dbl&gt;
## 1 (Intercept)  108.      88.4         1.22 0.245     
## 2 volume         0.709    0.0975      7.27 0.00000626
```
]
.pull-right[
&lt;img src="25-multiple-predictors-model-selection_files/figure-html/unnamed-chunk-5-1.png" width="75%" style="display: block; margin: auto 0 auto auto;" /&gt;
]

---

## Book weight vs. volume and cover

.pull-left[

``` r
lm(weight ~ volume + cover, data = allbacks) |&gt;
  tidy()
```

```
## # A tibble: 3 × 5
##   term        estimate std.error statistic      p.value
##   &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;        &lt;dbl&gt;
## 1 (Intercept)  198.      59.2         3.34 0.00584     
## 2 volume         0.718    0.0615     11.7  0.0000000660
## 3 coverpb     -184.      40.5        -4.55 0.000672
```
]
.pull-right[
&lt;img src="25-multiple-predictors-model-selection_files/figure-html/unnamed-chunk-7-1.png" width="75%" style="display: block; margin: auto 0 auto auto;" /&gt;
]

---

## Interpretation of estimates


```
## # A tibble: 3 × 5
##   term        estimate std.error statistic      p.value
##   &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;        &lt;dbl&gt;
## 1 (Intercept)  198.      59.2         3.34 0.00584     
## 2 volume         0.718    0.0615     11.7  0.0000000660
## 3 coverpb     -184.      40.5        -4.55 0.000672
```

--
- **Slope - volume:** *All else held constant*, for each additional cubic centimetre books are larger in volume, we would expect the weight to be higher, on average, by 0.718 grams.

--
- **Slope - cover:** *All else held constant*, paperback books are weigh, on average, by 184 grams less than hardcover books.

--
- **Intercept:** Hardcover books with 0 volume are expected to weigh 198 grams, on average. (Doesn't make sense in context.)




---

## Main vs. interaction effects

.question[
Suppose we want to predict weight of books from their volume and cover type 
(hardback vs. paperback). Do you think a model with main effects or 
interaction effects is more appropriate? Explain your reasoning.

**Hint:** Main effects would mean rate at which weight changes as volume 
increases would be the same for hardback and paperback books and interaction 
effects would mean the rate at which weight changes as volume 
increases would be different for hardback and paperback books.
]

---

&lt;img src="25-multiple-predictors-model-selection_files/figure-html/book-main-int-1.png" width="65%" style="display: block; margin: auto;" /&gt;

---

## In pursuit of Occam's razor

- Occam's Razor states that among competing hypotheses that predict equally well, the simplest one should be selected.

--
- Model selection follows this principle.

--
- We only want to add another variable to the model if the addition of that variable brings something valuable in terms of predictive power to the model.

--
- In other words, we prefer the simplest best model, i.e. **parsimonious** model.

---

.pull-left[
&lt;img src="25-multiple-predictors-model-selection_files/figure-html/unnamed-chunk-9-1.png" width="100%" style="display: block; margin: auto;" /&gt;
]
.pull-right[
.question[
Visually, which of the two models is preferable under Occam's razor?
]
]

---

## R-squared

- `\(R^2\)` is the percentage of variability in the response variable explained by 
the regression model.


``` r
glance(book_main_fit)$r.squared
```

```
## [1] 0.9274776
```

``` r
glance(book_int_fit)$r.squared
```

```
## [1] 0.9297137
```

--
- Clearly the model with interactions has a higher `\(R^2\)`.

--
- However using `\(R^2\)` for model selection in models with multiple explanatory variables is not a good idea as `\(R^2\)` increases when **any** variable is added to the model.

---

## Adjusted R-squared

... a (more) objective measure for model selection

- Adjusted `\(R^2\)` doesn't increase if the new variable does not provide any new 
informaton or is completely unrelated, as it applies a penalty for number of 
variables included in the model.
- This makes adjusted `\(R^2\)` a preferable metric for model selection in multiple
regression models.

---

## Comparing models

.pull-left[

``` r
glance(book_main_fit)$r.squared
```

```
## [1] 0.9274776
```

``` r
glance(book_int_fit)$r.squared
```

```
## [1] 0.9297137
```
]
.pull-right[

``` r
glance(book_main_fit)$adj.r.squared
```

```
## [1] 0.9153905
```

``` r
glance(book_int_fit)$adj.r.squared
```

```
## [1] 0.9105447
```
]

--

.pull-left-wide[
.small[
- Is R-sq higher for int model?

``` r
glance(book_int_fit)$r.squared &gt; glance(book_main_fit)$r.squared 
```

```
## [1] TRUE
```

- Is R-sq adj. higher for int model?


``` r
glance(book_int_fit)$adj.r.squared &gt; glance(book_main_fit)$adj.r.squared
```

```
## [1] FALSE
```
]
]

---

## Model Selection 

+ When you have many variables in your dataset, how do you choose which ones best predict/explain your outcome of interest?

+ `\(2^p\)` possible models for `\(p\)` predictors. (e.g. `\(2^{10} = 1024\)`)

+ Forward selection: fit all one-predictor models, choose "best" one, then fit all two-predictor models (with 1st predictor already determined), choose "best" one, etc

+ Backward selection: fit model with all possible predictors, drop variables one at a time until you arrive at "best" one

+ "Best subsets" selection: uses a combination of forward and backward selection to select a "best" model for each # of possible predictors (e.g. best 1-predictor model, best 2-predictor model, .... best 10-predictor model, ... )

---

## Model selection: Best subsets




``` r
models &lt;- regsubsets(logprice ~ ., data = pp_subset, nvmax = 10, method = "seqrep")
summary(models)$which[,1:6]
```

```
##    (Intercept) position dealerL dealerP dealerR  year
## 1         TRUE    FALSE   FALSE   FALSE    TRUE FALSE
## 2         TRUE    FALSE   FALSE   FALSE    TRUE  TRUE
## 3         TRUE    FALSE   FALSE   FALSE    TRUE  TRUE
## 4         TRUE    FALSE   FALSE   FALSE    TRUE  TRUE
## 5         TRUE    FALSE   FALSE   FALSE    TRUE  TRUE
## 6         TRUE    FALSE    TRUE   FALSE    TRUE  TRUE
## 7         TRUE    FALSE    TRUE   FALSE    TRUE  TRUE
## 8         TRUE    FALSE    TRUE   FALSE    TRUE  TRUE
## 9         TRUE    FALSE    TRUE   FALSE    TRUE  TRUE
## 10        TRUE    FALSE    TRUE   FALSE    TRUE  TRUE
```

``` r
summary(models)$adjr2
```

```
##  [1] 0.1478809 0.2909998 0.3429945 0.3893764 0.4186145 0.4397906
##  [7] 0.4443837 0.4525722 0.4547842 0.4574566
```

---

## "Best" model


``` r
summary(models)$which[7,]
```

```
##     (Intercept)        position         dealerL         dealerP 
##            TRUE           FALSE            TRUE           FALSE 
##         dealerR            year     origin_catF     origin_catI 
##            TRUE            TRUE           FALSE           FALSE 
##     origin_catO     origin_catS school_pntgD/FL    school_pntgF 
##           FALSE           FALSE            TRUE           FALSE 
##    school_pntgG    school_pntgI    school_pntgS    school_pntgX 
##           FALSE           FALSE           FALSE           FALSE 
##     diff_origin    artistliving       Height_in        Width_in 
##            TRUE           FALSE           FALSE            TRUE 
##         Surface          paired           relig        landsALL 
##            TRUE           FALSE           FALSE           FALSE 
##           mytho        portrait 
##           FALSE           FALSE
```

``` r
model_best &lt;- lm(logprice ~ dealer + year + school_pntg + diff_origin + 
                   Width_in + Surface, data = pp_subset)
```

---

## "Best" model: Adjusted `\(R^2 = 0.445\)`


```
## # A tibble: 14 × 5
##    term               estimate  std.error statistic   p.value
##    &lt;chr&gt;                 &lt;dbl&gt;      &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;
##  1 (Intercept)     -222.       10.1         -22.0   1.76e- 99
##  2 dealerL            1.03      0.0967       10.6   5.14e- 26
##  3 dealerP            0.200     0.122         1.64  1.01e-  1
##  4 dealerR            2.21      0.0794       27.9   5.11e-153
##  5 year               0.127     0.00565      22.5   5.70e-104
##  6 school_pntgD/FL   -0.190     0.984        -0.193 8.47e-  1
##  7 school_pntgF      -1.02      0.984        -1.03  3.01e-  1
##  8 school_pntgG      -0.675     1.16         -0.582 5.60e-  1
##  9 school_pntgI      -1.02      0.986        -1.03  3.02e-  1
## 10 school_pntgS      -0.637     1.11         -0.572 5.67e-  1
## 11 school_pntgX      -1.60      1.01         -1.59  1.12e-  1
## 12 diff_origin       -0.887     0.0670      -13.3   4.94e- 39
## 13 Width_in           0.0468    0.00342      13.7   2.64e- 41
## 14 Surface           -0.000249  0.0000489    -5.10  3.58e-  7
```

---

## Model diagnostics


``` r
model_best &lt;- lm(logprice ~ dealer + year + school_pntg + diff_origin + 
                   Width_in + Surface, data = pp)
model_best_aug &lt;- augment(model_best)

ggplot(data = model_best_aug, aes(x = .fitted, y = .resid)) +
  geom_point(alpha = 0.5)
```

&lt;img src="25-multiple-predictors-model-selection_files/figure-html/unnamed-chunk-19-1.png" width="60%" style="display: block; margin: auto;" /&gt;

    </textarea>
<style data-target="print-only">@media screen {.remark-slide-container{display:block;}.remark-slide-scaler{box-shadow:none;}}</style>
<script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
<script>var slideshow = remark.create({
"ratio": "16:9",
"highlightLines": true,
"highlightStyle": "solarized-light",
"countIncrementalSlides": false
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function(d) {
  var s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})(document);

(function(d) {
  var el = d.getElementsByClassName("remark-slides-area");
  if (!el) return;
  var slide, slides = slideshow.getSlides(), els = el[0].children;
  for (var i = 1; i < slides.length; i++) {
    slide = slides[i];
    if (slide.properties.continued === "true" || slide.properties.count === "false") {
      els[i - 1].className += ' has-continuation';
    }
  }
  var s = d.createElement("style");
  s.type = "text/css"; s.innerHTML = "@media print { .has-continuation { display: none; } }";
  d.head.appendChild(s);
})(document);
// delete the temporary CSS (for displaying all slides initially) when the user
// starts to view slides
(function() {
  var deleted = false;
  slideshow.on('beforeShowSlide', function(slide) {
    if (deleted) return;
    var sheets = document.styleSheets, node;
    for (var i = 0; i < sheets.length; i++) {
      node = sheets[i].ownerNode;
      if (node.dataset["target"] !== "print-only") continue;
      node.parentNode.removeChild(node);
    }
    deleted = true;
  });
})();
// add `data-at-shortcutkeys` attribute to <body> to resolve conflicts with JAWS
// screen reader (see PR #262)
(function(d) {
  let res = {};
  d.querySelectorAll('.remark-help-content table tr').forEach(tr => {
    const t = tr.querySelector('td:nth-child(2)').innerText;
    tr.querySelectorAll('td:first-child .key').forEach(key => {
      const k = key.innerText;
      if (/^[a-z]$/.test(k)) res[k] = t;  // must be a single letter (key)
    });
  });
  d.body.setAttribute('data-at-shortcutkeys', JSON.stringify(res));
})(document);
(function() {
  "use strict"
  // Replace <script> tags in slides area to make them executable
  var scripts = document.querySelectorAll(
    '.remark-slides-area .remark-slide-container script'
  );
  if (!scripts.length) return;
  for (var i = 0; i < scripts.length; i++) {
    var s = document.createElement('script');
    var code = document.createTextNode(scripts[i].textContent);
    s.appendChild(code);
    var scriptAttrs = scripts[i].attributes;
    for (var j = 0; j < scriptAttrs.length; j++) {
      s.setAttribute(scriptAttrs[j].name, scriptAttrs[j].value);
    }
    scripts[i].parentElement.replaceChild(s, scripts[i]);
  }
})();
(function() {
  var links = document.getElementsByTagName('a');
  for (var i = 0; i < links.length; i++) {
    if (/^(https?:)?\/\//.test(links[i].getAttribute('href'))) {
      links[i].target = '_blank';
    }
  }
})();
// adds .remark-code-has-line-highlighted class to <pre> parent elements
// of code chunks containing highlighted lines with class .remark-code-line-highlighted
(function(d) {
  const hlines = d.querySelectorAll('.remark-code-line-highlighted');
  const preParents = [];
  const findPreParent = function(line, p = 0) {
    if (p > 1) return null; // traverse up no further than grandparent
    const el = line.parentElement;
    return el.tagName === "PRE" ? el : findPreParent(el, ++p);
  };

  for (let line of hlines) {
    let pre = findPreParent(line);
    if (pre && !preParents.includes(pre)) preParents.push(pre);
  }
  preParents.forEach(p => p.classList.add("remark-code-has-line-highlighted"));
})(document);</script>

<script>
slideshow._releaseMath = function(el) {
  var i, text, code, codes = el.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
};
slideshow._releaseMath(document);
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
